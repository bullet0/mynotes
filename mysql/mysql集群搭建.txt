1.hadoop1 安装docker - mysql
	查看docker仓库
		docker search mysql
	版本号可以去docker hub官网搜索查看	
		https://hub.docker.com/
	启动一个mysql容器
		docker run --privileged=true --restart=always -d -p 3306:3306 --name mysql8 -v /mysql/mysql-files:/var/lib/mysql-files/ -v /mysql/conf/:/etc/mysql/ -v /mysql/logs:/var/log/mysql -v /mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 62a9 --default-authentication-plugin=mysql_native_password 
			-p 3306:3306：将容器的 3306 端口映射到主机的 3306 端口。
			-v /mysql/conf:/etc/mysql/ ：将主机当前目录下的 conf/my.cnf 挂载到容器的 /etc/mysql/my.cnf。
			-v /mysql/logs:/logs：将主机当前目录下的 logs 目录挂载到容器的 /logs。
			-v /mysql/data:/var/lib/mysql ：将主机当前目录下的data目录挂载到容器的 /var/lib/mysql 。
			-v /mysql/mysql-files:/var/lib/mysql-files/	：当指定了外部配置文件与外部存储路径时，也需要指定 /var/lib/mysql-files的外部目录
			-e MYSQL_ROOT_PASSWORD=123456：初始化 root 用户的密码。
			--restart=always ：当 Docker 重启时，容器能自动启动
			--default-authentication-plugin=mysql_native_password
				mysql8.0版本密码是加密连接的，设置成不加密连接，否则工具连不上  放在后面，同时首先保证/mysql下是空的，如果之前生成过一回，则会发现这句不生效
			--privileged=true    
				现象：启动mysql容器时自动停止   放在前面
				使用 [ docker logs 容器id ] 命令查看，发现提示信息是：chown: changing ownership of ‘/var/lib/mysql/....‘: Permission denied
			·其实挂载的目的是把数据备份在外部，保证容器丢失而数据不丢失，其他配置文件可以在容器内改变
			·8.0的配置文件my.cnf是没有的，所以挂载出来也找不到，但是你可以自己找一个配置文件放到你挂载的/mysql/conf下，起名my.cnf,启动容器即可成功读取配置文件
	同样的方式启动另一个mysql容器
		如果是单机，容器内的3306不用改,映射出来的3306改掉,同时启动的--name也改一个,挂载的配置文件位置也改一下即可,如果多台服务器,则无所谓
		docker run --privileged=true --restart=always -d -p 3307:3306 --name mysql88 -v /mysql1/mysql-files:/var/lib/mysql-files/ -v /mysql1/conf/:/etc/mysql/ -v /mysql1/logs:/var/log/mysql -v /mysql1/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 62a9 --default-authentication-plugin=mysql_native_password
	在两个库配置主从配置文件前,需要保证以下两种情况之一
		1.如果已有数据,应该先保证两边数据一致的基础上，或者数据结构一致的基础上，再配置主从,也就是需要将主库数据dump到从库一份
		2.两个新库，可以先配置主从，然后再新建数据
	配置两个mysql的主从复制的配置文件,放在挂载出来的外部位置即可
		主从复制原理:Mysql 中有一种日志叫做 bin 日志（二进制日志）。这个日志会记录下所有修改了数据库的SQL 语句（insert,update,delete,create/alter/drop table, grant 等等）。主从复制的原理其实就是把主服务器上的 bin 日志复制到从服务器上执行一遍，这样从服务器上的数据就和主服务器上的数据相同了。
		将主从内容放在[mysqld]下
		主库配置文件
			[mysqld]
			# 设置3306端口
			port=3306
			# 设置mysql的安装目录
			basedir=/usr
			# 设置mysql数据库的数据的存放目录
			datadir=/var/lib/mysql
			# 允许最大连接数
			max_connections=200
			# 允许连接失败的次数。这是为了防止有人从该主机试图攻击数据库系统
			max_connect_errors=10
			# 服务端使用的字符集默认为UTF8
			character-set-server=utf8
			# 创建新表时将使用的默认存储引擎
			default-storage-engine=INNODB
			# 默认使用“mysql_native_password”插件认证
			default_authentication_plugin=mysql_native_password

			#开启主从复制，主库的配置 
			log-bin=mysql-bin
			#指定主库serverid 
			server-id=1
			#指定同步的数据库，如果不指定则同步全部数据库 
			binlog-do-db=aaa
			# 三种binlog模式，建议使用混合模式，默认row，STATEMENT，ROW，MIXED
			binlog_format=MIXED

			[mysql]
			# 设置mysql客户端默认字符集
			default-character-set=utf8
			[client]
			# 设置mysql客户端连接服务端时默认使用的端口
			port=3306
			default-character-set=utf8

		从库配置文件
			[mysqld]
			# 设置3306端口
			port=3306
			# 设置mysql的安装目录
			basedir=/usr
			# 设置mysql数据库的数据的存放目录
			datadir=/var/lib/mysql
			# 允许最大连接数
			max_connections=200
			# 允许连接失败的次数。这是为了防止有人从该主机试图攻击数据库系统
			max_connect_errors=10
			# 服务端使用的字符集默认为UTF8
			character-set-server=utf8
			# 创建新表时将使用的默认存储引擎
			default-storage-engine=INNODB
			# 默认使用“mysql_native_password”插件认证
			default_authentication_plugin=mysql_native_password

			#指定从库serverid 暂时只配置了这一个
			server-id=2


			[mysql]
			# 设置mysql客户端默认字符集
			default-character-set=utf8
			[client]
			# 设置mysql客户端连接服务端时默认使用的端口
			port=3306
			default-character-set=utf8

	重启两个mysql
		主库mysql执行如下sql
			创建从库可在主库登陆的用户
			CREATE USER 'slave01'@'%' IDENTIFIED BY '123456'; 
			将用户授权
			GRANT REPLICATION SLAVE ON *.* TO 'slave01'@'%';
			刷新权限
			FLUSH PRIVILEGES;
			查看msater的状态,主要看position和file两列，等会在slave使用
			SHOW MASTER STATUS;
			查看日志状态:binlog是否打开
			SHOW GLOBAL VARIABLES LIKE '%log%';
			查看server_id
			SHOW GLOBAL VARIABLES LIKE '%server%';
		从库mysql执行如下sql
			设置msater相关信息,MASTER_HOST master主机ip, MASTER_USER 上面的用户名, MASTER_PASSWORD 上面的密码, MASTER_PORT 端口, MASTER_LOG_FILE 上面查到的file, MASTER_LOG_POS 上面查到的position
			CHANGE MASTER TO MASTER_HOST='192.168.64.133', MASTER_USER='slave01', MASTER_PASSWORD='123456', MASTER_PORT=3306, MASTER_LOG_FILE='mysql-bin.000002', MASTER_LOG_POS=701;
			如果已经启动，需要先停止
			STOP SLAVE;
			开启slave模式
			START SLAVE;
			查看slave状态，主要看slave_io_running 和 slave_sql_running 是否为yes
			SHOW SLAVE STATUS;
	之后就可以在主库操作数据，从库会自动同步，如果没有同步，请使用 [ docker logs 从库容器ID ] 查看从库的日志报错信息


2 hadoop1 制作mycat docker 镜像
	由于没有找到官方的mycat docker 镜像，所以自己根据网上教程搭建了一个自己的镜像，步骤如下
		1.随便创建一个空文件夹 mkdir mycat
		2.下载jdk1.8，放入上面目录
		3.下载mycat1.7.6，放入上面目录
		4.编写一个Dockerfile
			# 设置镜像的 base 镜像，这里我们使用 centos 系统镜像
			FROM docker.io/centos
			# 维护者信息
			MAINTAINER Mr.BULLET
			# 复制依赖的 jdk 文件，我这里是已经解压缩，如果是 tar 格式文件，使用 ADD
			# COPY jdk1.8.0_162 /opt/jdk1.8.0_162
			ADD jdk-8u221.tar.gz /usr/local/
			# 复制 mycat 文件，我这里是已经解压缩，如果是 tar 或 gz 格式文件，使用 ADD
			# COPY mycat1.65 /opt/mycat1.65
			ADD mycat-server-1.6.7.3.tar.gz  /usr/local/
			# 设置环境变量
			ENV JAVA_HOME=/usr/local/jdk1.8.0_221
			ENV PATH=$JAVA_HOME/bin:$PATH
			ENV CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
			ENV MYCAT_HOME=/usr/local/mycat
			# mycat的两个端口
			EXPOSE 8066 9066
			RUN chmod -R 777 /usr/local/mycat/
			RUN source /etc/profile
			RUN source ~/.bash_profile
			# 执行最终命令，启动 mycat
			CMD ["/usr/local/mycat/bin/mycat", "console"]
		5.tree mycat 看下是否符合如下目录结构
			.(mycat)
			├── Dockerfile
			├── jdk-8u221.tar.gz
			└── mycat-server-1.6.7.3.tar.gz
		6.在mycat目录下，执行docker打包构建命令
			docker build -t 仓库名:标签           Dockerfile文件路径
			docker build -t bullet/mycat:1.6.7.3 .
		7.构建完成后，使用docker images查看
		8.上传docker hub仓库
			首先你得有一个docker hub，自己注册，然后创建一个仓库，起一个仓库名，docker hub会自动把你的用户名和仓库名拼起来作为二级路径
			比如我的用户名是 bulletbullet ,仓库名为 mycat   ，所以我的二级路径就是 bulletbullet/mycat
			为什么叫二级路径，因为docker规定所有的路径都在docker.io下，所以其实你的仓库路径是 /docker.io/bulletbullet/mycat ,不管你怎么写都会自动到docker.io下找，所以你自己的只能叫二级路径
			这个二级路径很重要，一会就是要把这个镜像传入这个路径下
		9.登陆docker hub,会让你输入dockerhub的用户名密码登陆
			docker login
		10.上传镜像
			docker push 仓库名:标签
			可以看到我上面build的时候写的是  bullet/mycat:1.6.7.3   意味着我的仓库名为 /docker.io/bullet/mycat , 标签为  1.6.7.3
			所以我要是直接push这个镜像，就会报找不到路径之类的错误，反正是传不上去
			所以我们需要改名，使用 [ docker tag ] 命令改名,再开一个镜像,这个名就得是你自己的  二级路径:标签  标签随便，二级路径必须对
				docker tag bullet/mycat:1.6.7.3 bulletbullet/mycat:1.6.7.3
			然后 
				docker push bulletbullet/mycat:1.6.7.3
			就可以成功push了
		11.拉自己的镜像
			docker pull bulletbullet/mycat:1.6.7.3
			会拉下来一个仓库在 docker.io/bulletbullet/mycat 的镜像

	3 启动mycat镜像
		docker run --name mycat -p 8066:8066 -p 9066:9066 -v /mycat/conf/:/usr/local/mycat/conf/ --privileged=true -d 镜像ID
		然后发现起不来（可以docker logs 查看一下）
		因为mycat运行时候会访问mycat/conf/wrapper.conf,所以在挂载出来的目录下放置wrapper.conf（这个文件去压缩包里拿就行）
		然后就能起来了
		使用ip:8066就能连接到一个数据库，这个库就是mycat自带的逻辑库

		--privileged=true   进入容器后发现root用户没有权限访问某些目录，run的时候加上这个就行了

	4.测试mycat是否正常启动
		使用工具连接8066端口,默认用户名密码root:123456,如果能连上，说明没问题，但是不要去查询表里数据，因为它配置了表里数据从一个固定地方取，但这个地方还不存在，所以会报错，这是正常的，能连上就没问题，还可以用docker logs 查看日志
		默认的用户名密码可以到server.xml中看

	5.配置主从复制，读写分离
		mycat分有逻辑库和真实库，逻辑库就是你连上mycat后看到的库，真实库是你mysql那个库，逻辑库想要显示真实库，那就需要打通mycat连接mysql；我们之后直接操作的是逻辑库，所以我们也要能登陆mycat，配置就是配置这些东西
		主要配置文件如下
			server.xml: 配置mycat的核心配置,比如内存几个G啊,还有登陆mycat的用户名密码,以及登陆之后能看到哪个逻辑库
			schema.xml: 配置逻辑库和真实库之间的连接关系
			rule.xml: 分库分表中各种规则配置			
		这里只说读写分离，读写分离基础是一主一从，写主读从
		首先配置server.xml
			其他配置暂时不管，我们只需配一个我们登陆的用户就行，表示用root登陆就能看到一个aaa的逻辑库，名字随便起
				<user name="root">
					<property name="password">123456</property>
					<property name="schemas">aaa</property>
        		</user>
		接着配置schema.xml,就是这个aaa逻辑库去哪取数据
			<?xml version="1.0"?>
			<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
			<mycat:schema xmlns:mycat="http://io.mycat/">
				<!-- 
					name:逻辑库名，要跟server.xml中一致
					sqlMaxLimit: 如果不带limit,默认限制显示多少条
				-->
				<schema name="aaa" checkSQLschema="true" sqlMaxLimit="100">
						<!--
							name: 真实表表名
							dataNode: 数据分布在哪些节点上,可配置多个,名字随便起  dn1,dn2,dn3
							rule: 使用rule.xml中哪个分库规则
						-->
						<table name="zqp" dataNode="dn1" rule="mod-long" />
				</schema>

				<!-- 
					此标签可配置多个,用于声明上面的dataNode
					name: 跟上面指定的dataNode对应
					dataHost: 这个node要去哪个mysql主机取数据,别名,可以随便起
					database: 去主机的哪个数据库中取数据
				-->
				<dataNode name="dn1" dataHost="master" database="aaa" />
				
				<!-- 
					这个标签用来声明上面的dataHost
					name: 跟上面对应
					balance: 0	不开启读写分离机制，所有读操作都发送到当前可用的writeHost上
							 1	全部的readHost与stand by writeHost参与select语句的负载均衡，简单的说，当双主双从模式（M1-S1，M2-S2 并且M1 M2互为主备），正常情况下，M2,S1,S2都参与select语句的负载均衡。
							 2	所有读操作都随机的在writeHost、readHost上分发
							 3	所有读请求随机的分发到writeHst对应的readHost执行，writeHost不负担读写压力。（mycat1.4之后版本才有）
							 这个才是读写分离核心,建议3
					writeType: 0	所有写操作发送到配置的第一个 writeHost ，第一个挂了切到还生存的第二个writeHost，重新启动后已切换后的为准，切换记录在配置文件中:dnindex.properties .
							   1	所有写操作都随机的发送到配置的 writeHost。1.5以后版本废弃不推荐。

					switchType:	1	默认值 自动切换
							    2 	基于MySql主从同步的状态决定是否切换心跳语句为 show slave status
								3	基于mysql galary cluster 的切换机制（适合集群）1.4.1 心跳语句为 show status like 'wsrep%'
					
					dbType:	指定后端链接的数据库类型目前支持二进制的mysql协议，还有其他使用jdbc链接的数据库，例如：mongodb，oracle，spark等
					dbDriver: 指定连接后段数据库使用的driver，目前可选的值有native和JDBC。使用native的话，因为这个值执行的是二进制的mysql协议，所以可以使用mysql和maridb，其他类型的则需要使用JDBC驱动来支持。
							  如果使用JDBC的话需要符合JDBC4标准的驱动jar 放到mycat\lib目录下，并检查驱动jar包中包括如下目录结构文件 META-INF\services\java.sql.Driver。 在这个文件写上具体的driver类名，例如com.mysql.jdbc.Driver
				-->
				<dataHost name="master" maxCon="1000" minCon="10" balance="3"
								writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
						<!-- 心跳语句 -->
						<heartbeat>select user()</heartbeat>
						<!-- 
							写库主机配置，主库
							can have multi write hosts 
							host: 随便起，别重复就行
							url: mysql的地址:端口
							user: 登陆用户名
							password: 登陆密码
						-->
						<writeHost host="hostM1" url="192.168.64.133:3306" user="root"
										password="123456">
								<!-- 
									读库主机配置，从库
									can have multi read hosts 
									host: 随便起，别重复就行
									url: mysql的地址:端口
									user: 登陆用户名
									password: 登陆密码
									--weight	权重 配置在 readhost 中作为读节点的权重
									--usingDecrypt	是否对密码加密，默认0。具体加密方法看官方文档。
								-->
								<readHost host="hostS2" url="192.168.64.133:3307" user="root" password="123456" />
						</writeHost>
				</dataHost>
			</mycat:schema>
		接着配置rule.xml
			因为我就配置使用mod-long规则，所以以此演示
			一共两组标签 tableRule 和 function
			<!--
				name: 对应schema.xml中的rule
				columns: 对应真实表中的列：用哪一列分表
				algorithm：分表算法，具体对应下面的function
			-->
			<tableRule name="mod-long">
                <rule>
                        <columns>id</columns>
                        <algorithm>mod-long</algorithm>
                </rule>
        	</tableRule>
			<!--
				用来声明算法，提供给上面引用
				name：对应tableRule中algorithm引用的的算法名字
				class：java写的,算法类路径
				里面配置1是我改的，默认3，因为我只有一主一从，所以这个只能分到一台机器，一般有几台写主机就配几
			-->
			<function name="mod-long" class="io.mycat.route.function.PartitionByMod">
                <!-- how many data nodes -->
                <property name="count">1</property>
        	</function>
	6.配置mycat集群
		多部署几台即可

HAproxy配置
	1.拉HAproxy的镜像
		docker pull docker.io/haproxy
	2.启动HAproxy
		docker run -it -d -p 4001:8888 -p 4002:3306  --privileged=true  -v /haproxy/conf:/usr/local/etc/haproxy --name haproxy  haproxy
		同样需要先有配置文件
		在conf下新建一个haproxy.cfg,内容如下
			global
			 #工作目录
			 chroot /usr/local/etc/haproxy
			 pidfile /var/run/haproxy.pid
			 #日志文件，使用rsyslog服务中local5日志设备（/var/log/local5），等级info
			 log 127.0.0.1 local5 info
			 maxconn 4000
			 #守护进程运行
			 daemon

			defaults
			 #默认的模式mode { tcp|http|health }，tcp是4层，http是7层，health只会返回OK
			 mode http
			 log global
			 #日志格式
			 option httplog
			 #日志中不记录负载均衡的心跳检测记录
			 option dontlognull
			 option http-server-close
			 #option forwardfor except 127.0.0.0/8
			 #当serverId对应的服务器挂掉后，强制定向到其他健康的服务器
			 option redispatch
			 #两次连接失败就认为是服务器不可用，也可以通过后面设置
			 retries 3
			 #当服务器负载很高的时候，自动结束掉当前队列处理比较久的链接
			 option abortonclose
			 timeout http-request 10s
			 timeout queue 1m
			 timeout connect 10s
			 timeout client 1m
			 timeout server 1m
			 timeout http-keep-alive 10s
			 timeout check 10s
			 maxconn 3000

			listen admin_stats
			 #监控界面的访问的IP和端口
			 bind 0.0.0.0:8888
			 mode http
			 #URI相对地址
			 stats uri /dbs
			 #统计报告格式
			 stats realm Global\ statistics
			 #登陆帐户信息
			 stats auth admin:admin123
			listen proxy-mysql
			 #访问的IP和端口(前面ip=0代表任何ip都可访问)
			 bind 0.0.0.0:3306
			 #网络协议      
			 mode tcp
			 #负载均衡算法（轮询算法）
			 #轮询算法：roundrobin
			 #权重算法：static-rr
			 #最少连接算法：leastconn
			 #请求源IP算法：source
			 balance roundrobin
			 #日志格式
			 option tcplog
			 
			 #在MySQL中创建一个没有权限的haproxy用户，密码为空。Haproxy使用这个账户对MySQL数据库心跳检测
			 #这个要自己去数据库中创建
			 # create user 'haproxy'@'%' identified by ''; FLUSH PRIVILEGES;
			 # mycat没有找到配置这个的地方，所以直接注释掉，HAproxy可以正常跑
			 #option mysql-check user haproxy

			 #代理mycat服务或者mysql服务，可配置多个
			 server mycat_1 192.168.64.133:8066  check  port  8066  maxconn  2000 weight 1
			 #server   mycat_2  192.168.1.18:8067  check  port  8067  maxconn  2000
			 #使用keepalive检测死链
			 #option        tcpka
	3.监控页面
		http://192.168.64.133:4001/dbs
		docker run的时候4001绑定到容器内部的8888，容器内部haproxy监听8888端口
	4.工具连接
		用户名密码是mysql配的那个用户名面
		ip是haproxy的IP
		端口是4002  docker run 的时候4002绑定到容器内部的3306，容器内部haproxy监听3306

在HAproxy外套一层keepalived
	keepalived的实现是要在HAproxy-docker服务内创建，最后宿主机中也要创建实现docker内外的连接
	原因，整个的架构是在HAproxy外包裹一层keepalived，keepalived依据额外的脚本与HAproxy建立联系，而keepalived是在服务器上创建一个虚拟IP（VIP），这样就把VIP创建在了docker容器中
	docker外部宿主机可以访问容器内部的VIP,但是外界客户端无法直接访问内部VIP，所以在宿主机中再建立一个keepalived，用来转发请求到容器内部
	1.在容器内安装keepalived
		更新apt-get，安装keepalived
		apt-get update
		apt-get install keepalived
		安装vim 
		apt-get install vim
		安装ifconfig命令 安装ping
		apt-get install net-tools    
		apt-get install iputils-ping
		安装ps命令，脚本里会用到
		apt-get install procps
	2.编辑配置文件
		vim /etc/keepalived/keepalived.conf
			全局配置
			global_defs {
				notification_email {   定义报警邮件地址
				acassen@firewall.loc
				failover@firewall.loc
				sysadmin@firewall.loc
				} 
				notification_email_from Alexandre.Cassen@firewall.loc  #定义发送邮件的地址
				smtp_server 192.168.200.1   #邮箱服务器 
				smtp_connect_timeout 30      #定义超时时间
				router_id LVS_DEVEL        #定义路由标识信息，相同局域网唯一
			}  
			#定义检查脚本
			vrrp_script check_haproxy {
				script "/etc/keepalived/chk_haproxy.sh"
				interval 2
				weight 2
			}
			#定义一个vrrp的实例块，后面接上实例块的名称，可以任意起，最好事字符串，这里我们定义的是VI_1
			vrrp_instance VI_1 {   
				#状态参数 只有两个选择  MASTER/BACKUP  MASTER需要把优先级配高
				state MASTER
				interface eth0       #指定虚拟IP定义在那个网卡上面,通过本机的ifconfig查看
				virtual_router_id 51 #虚拟路由ID，负责相同虚拟IP的keepalived集群最好定义为相同的id
				priority 100         # 优先级决定是主还是备    越大越优先
				advert_int 1        #主备通讯时间间隔
				# 准备服务器之间的认证类型与额外指定
				authentication {
					auth_type PASS    #认证方式 PASS 表示使用密码认证
					auth_pass 1111    #密码
				} 
				# 定义虚拟IP块  需要和本机在同一个网段，通过本机的ifconfig查看本机网段
				virtual_ipaddress {
					172.17.0.100    #设备之间使用的虚拟ip地址
				}
				#执行检查脚本
				track_script {
					check_haproxy
				}
			}
		注意:一开始配置可以删掉全局配置和haproxy脚本有关的两段配置，单独启动keepalived
			Keepalived主备配置文件区别：
			01. router_id 信息不一致
			02. state 状态描述信息不一致
			03. priority 主备竞选优先级数值不一致

	3.启动keepalived
		service keepalived start
	4.查看是否启动
		ps -aux | grep keepalived
		如果没有启动可以查看是否配置文件有问题
		keepalived -t -f /etc/keepalived/keepalived.conf
	5.查看VIP是否生成
		ip addr

		没有VIP之前的样子
		1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
			link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
			inet 127.0.0.1/8 scope host lo
			valid_lft forever preferred_lft forever
			inet6 ::1/128 scope host 
			valid_lft forever preferred_lft forever
		12: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
			link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0
			inet 172.17.0.5/16 scope global eth0
			valid_lft forever preferred_lft forever
			inet6 fe80::42:acff:fe11:5/64 scope link 
			valid_lft forever preferred_lft forever
		配置成功后的样子
		1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
			link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
			inet 127.0.0.1/8 scope host lo
			valid_lft forever preferred_lft forever
			inet6 ::1/128 scope host 
			valid_lft forever preferred_lft forever
		12: eth0@if13: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
			link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0
			inet 172.17.0.5/16 scope global eth0
			valid_lft forever preferred_lft forever
			inet 172.17.0.100/32 scope global eth0
			valid_lft forever preferred_lft forever
			inet6 fe80::42:acff:fe11:5/64 scope link 
			valid_lft forever preferred_lft forever
		只要看到你网卡上多了一个你配置文件中写的ip，就对了
			inet 172.17.0.100/32 scope global eth0
			valid_lft forever preferred_lft forever
	6.检查是否正常ping通
		容器内部ping VIP
		容器外部(宿主机)ping 查看VIP
		两个都通了说明没问题
			有问题的话
				1.先检查配置文件是否有问题，然后检查防火墙设置，可以重写一下配置文件，把多余空格/制表符都删掉，这个用-t好像检查不出来
				2.一定要注意容器run的时候需要带--privileged=true，这样可以获取到容器内的root权限，保证你的keepalived所有设置都没有权限干扰
				3.自行百度
配置HAproxy和keepalived互通
	这两个是两个软件，一个是用来建VIP的，一个是mycat代理的，互相没有关系，所以haproxy挂了keepalived是不关心的
	我们要做的就是让这两个工具绑定在一块，如果haproxy挂了，让keepalived也挂掉，放弃VIP，让别的keepalived去抢
	1.编写检查haproxy的脚本
		vim /etc/keepalived/chk_haproxy.sh

		#!/bin/bash
		Haproxy_Status=`ps -C haproxy --no-header |wc -l`
		if [ $Haproxy_Status -eq 0 ];then
		 /usr/local/sbin/haproxy -f /usr/local/etc/haproxy/haproxy.cfg
		 sleep 3
		 if [ `ps -C haproxy --no-header |wc -l` -eq 0 ];then
			/etc/init.d/keepalived stop
		 fi
		fi

	2.修改keepalived.conf配置文件
		vim /etc/keepalived/keepalived.conf
		增加上面删掉的两个跟haproxy有关系的配置就行
	3.重新启动keepalived
		service keepalived restart
	4.依照上面的方式检查即可

	5.设置keepalived开机启动，好像没有用
		安装chkconfig命令
		apt-get install sysv-rc-conf
		cp /usr/sbin/sysv-rc-conf /usr/sbin/chkconfig
		查看当前状态
		chkconfig -list
		如果有keepalived开启开机启动
		chkconfig keepalived on

配置宿主机的keepalived和内部互通
	安装keepalived
		yum install -y keepalived
	修改配置文件
		vim /etc/keepalived/keepalived.conf
			vrrp_instance VI_1 {
				state MASTER
				#这里是宿主机的网卡，可以通过ip a查看当前自己电脑上用的网卡名是哪个
				interface ens33
				virtual_router_id 100
				priority 100
				advert_int 1
				authentication {
				auth_type PASS
				auth_pass 1111
				}
				virtual_ipaddress {
				  #这里是指定的一个宿主机上的虚拟ip，一定要和宿主机网卡在同一个网段，我的宿主机网卡ip是192.168.64.133，所以指定虚拟ip是给的200
				  192.168.64.200
				}
			}
			#接受监听数据来源的端口，网页入口使用
			virtual_server 192.168.64.200 8888 {
				delay_loop 3
				lb_algo rr
				lb_kind NAT
				persistence_timeout 50
				protocol TCP
				#把接受到的数据转发给docker服务的网段及端口，由于是发给docker服务，所以和docker服务数据要一致
				real_server 172.17.0.100 8888 {
				weight 1
				}
			}

			#接受数据库数据端口，宿主机数据库端口是3310，所以这里要将3310转到容器的3306上
			virtual_server 192.168.64.200 3310 {
				delay_loop 3
				lb_algo rr
				lb_kind NAT
				persistence_timeout 50
				protocol TCP
				#同理转发数据库给服务的端口和ip要求和docker服务中的数据一致
				real_server 172.17.0.100 3306 {
				weight 1
				}
			}
	启动keepalived
		systemctl restart keepalived.service
	访问测试
		访问 VIP:8888/dbs  这里是192.168.64.200:8888/dbs,可以看到转到了haproxy管理页面说明页面代理成功
		用工具连接VIP端口3310，可以连上数据库内容和haproxy连接是一致的，可以CRUD，说明数据库代理成功 

将容器打包成镜像
	我们将haproxy容器中新增了keepalived，如果想把这个容器保存起来就需要将其打包成镜像，然后push到dockerhub上
	首先在dockerhub上新建一个仓库
		bulletbullet/keepalivedhaproxy
	提交容器成为镜像
		docker commit containerId 仓库二级路径
		docker commit containerId bulletbullet/keepalivedhaproxy
	查看镜像列表
		docker images 
	上传镜像	
		docker push dockerUserName/xxx[:tag]
	


MHA的优势很明显：开源，用Perl开发，代码结构清晰，二次开发容易；方案成熟，故障切换时，MHA会做到较严格的判断，尽量减少数据丢失，保证数据一致性；提供一个通用框架，可根据自己的情况做自定义开发，尤其是判断和切换操作步骤；支持binlog server，可提高binlog传送效率，进一步减少数据丢失风险。不过MHA也有些限制：需要在各个节点间打通ssh信任，这对某些公司安全制度来说是个挑战，因为如果某个节点被黑客攻破的话，其他节点也会跟着遭殃；自带提供的脚本还需要进一步补充完善，当然了，一般的使用还是够用的。

而PXC的优点有：服务高可用；数据同步复制(并发复制)，几乎无延迟；多个可同时读写节点，可实现写扩展，不过较好事先进行分库分表，让各个节点分别写不同的表或者库，避免让galera解决数据冲突；新节点可以自动部署，部署操作简单；数据严格一致性，尤其适合电商类应用；完全兼容MySQL。虽然PXC有这么多好处，但也有些局限性：只支持InnoDB引擎；PXC集群一致性控制机制，事有可能被终止；写入效率取决于节点中最弱的一台；不支持LOCK TABLE等显式锁操作；锁冲突、死锁问题相对更多；不支持XA；集群吞吐量/性能取决于短板；新加入节点采用SST时代价高。





		
		



        

	